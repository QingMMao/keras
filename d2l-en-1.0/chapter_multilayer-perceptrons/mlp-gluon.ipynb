{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concise Implementation of Multilayer Perceptron\n",
    "\n",
    "Now that we learned how multilayer perceptrons (MLPs) work in theory, let's implement them. We begin, as always, by importing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "The only difference from our softmax regression implementation\n",
    "is that we add two `Dense` (fully-connected) layers instead of one.  \n",
    "The first is our hidden layer, which has *256* hidden units \n",
    "and uses the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above we can invoke `net.add()` multiple times in succession,\n",
    "but we can also invoke it a single time, passing in \n",
    "multiple layers to be added the network. \n",
    "Thus, we could have equivalently written \n",
    "`net.add(nn.Dense(256, activation='relu'), nn.Dense(10))`. \n",
    "Again, note that as always, Gluon automatically \n",
    "infers the missing input dimensions to each layer. \n",
    "\n",
    "Training the model follows the exact same steps as in our softmax regression implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7956, train acc 0.707, test acc 0.824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 0.4916, train acc 0.817, test acc 0.853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.4323, train acc 0.841, test acc 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.4057, train acc 0.851, test acc 0.865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.3773, train acc 0.861, test acc 0.866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.3565, train acc 0.868, test acc 0.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 0.3444, train acc 0.872, test acc 0.872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 0.3280, train acc 0.880, test acc 0.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 0.3192, train acc 0.882, test acc 0.878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 0.3091, train acc 0.887, test acc 0.883\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})\n",
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None,\n",
    "              None, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Try adding a few more hidden layers to see how the result changes.\n",
    "1. Try out different activation functions. Which ones work best?\n",
    "1. Try out different initializations of the weights.\n",
    "\n",
    "## Scan the QR Code to [Discuss](https://discuss.mxnet.io/t/2340)\n",
    "\n",
    "![](../img/qr_mlp-gluon.svg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}